{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67db9d37-5e1f-4257-9829-c467ead4e083",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8823c72a-f71b-4259-9ee0-e985d3e60a02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "#import gpu_utils as g\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from model import PointNet_Plus#,Attension_Point,TVLAD\n",
    "from utils import group_points_4DV_T_S\n",
    "from dataset import NTU_RGBD\n",
    "import train\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f4967d-3914-47f1-be88-e78d92f96258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "#设置中文显示\n",
    "from pylab import mpl\n",
    "def curve(epoch,y,name1):\n",
    "    mpl.rcParams['font.sans-serif'] = ['SimHei'] \n",
    "    x=list(range(epoch))\n",
    "    plt.figure(figsize=(20,8),dpi=100)\n",
    "    plt.plot(x,y,color='r',linestyle='--')\n",
    "    #设置刻度[::5]切片\n",
    "    plt.xticks(x[::5])\n",
    "    plt.yticks(y[::5])\n",
    "    #添加描述信息\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(name1)\n",
    "    plt.title(name1)\n",
    "    #添加网格线\n",
    "    plt.grid(True,linestyle='--',alpha=0.5)\n",
    "    #4.保存图片\n",
    "    name='result/'+name1\n",
    "    plt.savefig(name)\n",
    "    #5.显示图像(4,5步顺序不能调换，如果4,5步调换顺序，则图片无法保存）\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc7788b-4558-4511-9172-0203cb7ce280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(Loss,Correct,Accuracy):\n",
    "    parser = argparse.ArgumentParser(description = \"Training\")  #定义命令行解析器对象\n",
    "    #添加命令行参数\n",
    "    parser.add_argument('--batchSize', type=int, default=8, help='input batch size')#￥￥￥￥\n",
    "    parser.add_argument('--nepoch', type=int, default=150, help='number of epochs to train for')\n",
    "    parser.add_argument('--INPUT_FEATURE_NUM', type=int, default = 3,  help='number of input point features')\n",
    "    parser.add_argument('--temperal_num', type=int, default = 3,  help='number of input point features')\n",
    "    parser.add_argument('--pooling', type=str, default='concatenation', help='how to aggregate temporal split features: vlad | concatenation | bilinear')\n",
    "    parser.add_argument('--dataset', type=str, default='ntu60', help='how to aggregate temporal split features: ntu120 | ntu60')\n",
    "\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0008, help='weight decay (SGD only)')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='learning rate at t=0')#￥￥￥￥\n",
    "    parser.add_argument('--gamma', type=float, default=0.5, help='')#￥￥￥￥\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='momentum (SGD only)')\n",
    "    parser.add_argument('--workers', type=int, default=0, help='number of data loading workers')\n",
    "\n",
    "    #parser.add_argument('--root_path', type=str, default='C:\\\\Users\\\\Administrator\\\\Desktop\\\\LX\\\\paper\\\\dataset\\\\Prosessed_dataset\\\\01_MSR3D',  help='preprocess folder')\n",
    "    parser.add_argument('--root_path', type=str, default='4DV/train_T',  help='preprocess folder')#预处理文件夹\n",
    "    # parser.add_argument('--depth_path', type=str, default='C:\\\\Users\\\\Administrator\\\\Desktop\\\\LX\\paper\\\\dataset\\\\Prosessed_dataset\\\\01_MSR3D\\\\',  help='raw_depth_png')\n",
    "    ################\n",
    "    # parser.add_argument('--save_root_dir', type=str, default='C:\\\\Users\\\\Administrator\\\\Desktop\\\\LX\\\\paper\\\\code\\\\3DV-Action-master\\\\models\\\\ntu60\\\\xsub',  help='output folder')\n",
    "    parser.add_argument('--save_root_dir', type=str, default='xsub_ntu',  help='output folder')#输出路径\n",
    "    parser.add_argument('--model', type=str, default = '',  help='model name for training resume')\n",
    "    parser.add_argument('--optimizer', type=str, default = '',  help='optimizer name for training resume')\n",
    "    \n",
    "    parser.add_argument('--ngpu', type=int, default=1, help='# GPUs')\n",
    "    parser.add_argument('--main_gpu', type=int, default=0, help='main GPU id') # CUDA_VISIBLE_DEVICES=0 python train.py\n",
    "\n",
    "    ########\n",
    "    parser.add_argument('--Seg_size', type=int, default =1,  help='number of frame in seg')\n",
    "    parser.add_argument('--stride', type=int, default = 1,  help='stride of seg')\n",
    "    parser.add_argument('--all_framenum', type=int, default = 32,  help='number of action frame')\n",
    "    parser.add_argument('--framenum', type=int, default = 32,  help='number of action frame')\n",
    "    parser.add_argument('--EACH_FRAME_SAMPLE_NUM', type=int, default = 512,  help='number of sample points in each frame')\n",
    "    parser.add_argument('--T_knn_K', type=int, default = 48,  help='K for knn search of temperal stream')\n",
    "    parser.add_argument('--T_knn_K2', type=int, default = 16,  help='K for knn search of temperal stream')\n",
    "    parser.add_argument('--T_sample_num_level1', type=int, default = 128,  help='number of first layer groups')\n",
    "    parser.add_argument('--T_sample_num_level2', type=int, default = 32,  help='number of first layer groups')\n",
    "    parser.add_argument('--T_ball_radius', type=float, default=0.2, help='square of radius for ball query of temperal stream')\n",
    "    \n",
    "    parser.add_argument('--learning_rate_decay', type=float, default=1e-7, help='learning rate decay')\n",
    "\n",
    "    parser.add_argument('--size', type=str, default='full', help='how many samples do we load: small | full')\n",
    "    parser.add_argument('--SAMPLE_NUM', type=int, default = 4096,  help='number of sample points')\n",
    "\n",
    "    parser.add_argument('--Num_Class', type=int, default = 6,  help='number of outputs')\n",
    "    parser.add_argument('--knn_K', type=int, default = 64,  help='K for knn search')\n",
    "    parser.add_argument('--sample_num_level1', type=int, default = 512,  help='number of first layer groups')\n",
    "    parser.add_argument('--sample_num_level2', type=int, default = 128,  help='number of second layer groups')\n",
    "    parser.add_argument('--ball_radius', type=float, default=0.1, help='square of radius for ball query in level 1')#0.025 -> 0.05 for detph\n",
    "    parser.add_argument('--ball_radius2', type=float, default=0.2, help='square of radius for ball query in level 2')# 0.08 -> 0.01 for depth\n",
    "\n",
    "\n",
    "    opt = parser.parse_args(args=[])\n",
    "    print(opt)\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%Y/%m/%d %H:%M:%S', filename=os.path.join(opt.save_root_dir, 'train00.log'), level=logging.INFO)\n",
    "    # torch.cuda.set_device(opt.main_gpu)\n",
    "\n",
    "    opt.manualSeed = 1\n",
    "    random.seed(opt.manualSeed)\n",
    "    torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(opt.save_root_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.empty_cache()\n",
    "    ##############################\n",
    "    data_train = NTU_RGBD(root_path = opt.root_path,opt=opt,\n",
    "        DATA_CROSS_VIEW = False,\n",
    "        full_train = True,\n",
    "        validation = False,\n",
    "        test = False,\n",
    "        Transform = True\n",
    "        )\n",
    "    train_loader = DataLoader(dataset = data_train, batch_size = opt.batchSize, shuffle = True, drop_last = True,num_workers = 8)\n",
    "    #print(data_train)\n",
    "    data_val = NTU_RGBD(root_path = opt.root_path, opt=opt,\n",
    "        DATA_CROSS_VIEW = False,\n",
    "        full_train = False,\n",
    "        validation = False,\n",
    "        test = True,\n",
    "        Transform = False\n",
    "        )\n",
    "    val_loader = DataLoader(dataset = data_val, batch_size = 24,num_workers = 8)\n",
    "\n",
    "    netR = PointNet_Plus(opt)\n",
    "\n",
    "    netR = torch.nn.DataParallel(netR).cuda()\n",
    "    netR.cuda()\n",
    "    print(netR)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.Adam(netR.parameters(), lr=opt.learning_rate, betas = (0.5, 0.999), eps=1e-06)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=opt.gamma)\n",
    "\n",
    "    for epoch in range(opt.nepoch):\n",
    "        scheduler.step()\n",
    "        #print(epoch)\n",
    "        # switch to train mode\n",
    "        torch.cuda.synchronize()\n",
    "        netR.train()\n",
    "        acc = 0.0\n",
    "        loss_sigma = 0.0\n",
    "        total1 = 0.0\n",
    "        timer = time.time()\n",
    "        \n",
    "        for i, data in enumerate(tqdm(train_loader, 0)):\n",
    "            if len(data[0])==1:\n",
    "                continue\n",
    "            torch.cuda.synchronize()\n",
    "            # 1 load imputs and target\n",
    "            ## 3DV points and 3 temporal segment appearance points\n",
    "            ## points_xyzc: B*4096*8;points_1xyz:B*2048*3  target: B*1\n",
    "            points4DV_T,label,v_name = data\n",
    "            points4DV_T,label = points4DV_T.cuda(),label.cuda()\n",
    "            # print('points4DV_T:',points4DV_T.shape)\n",
    "            xt, yt = group_points_4DV_T_S(points4DV_T, opt)#B*F*4*Cen*K  B*F*4*Cen*1\n",
    "            # print('xt:',xt.shape)\n",
    "            xt = xt.type(torch.FloatTensor)\n",
    "            yt = yt.type(torch.FloatTensor)\n",
    "\n",
    "            prediction = netR(xt,yt)\n",
    "\n",
    "            loss = criterion(prediction,label)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            # update training error\n",
    "            loss_sigma += loss.item()\n",
    "            #_, predicted60 = torch.max(prediction.data[:,0:60], 1)\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            # print(predicted.data)\n",
    "            acc += (predicted==label).cpu().sum().numpy()\n",
    "            total1 += label.size(0)\n",
    "        \n",
    "        \n",
    "        acc_avg = acc/total1\n",
    "        loss_avg = loss_sigma/total1\n",
    "        print('======>>>>> Online epoch: #%d, lr=%.10f,Acc=%f,correctnum=%f,allnum=%f,avg_loss=%f  <<<<<======' %(epoch, scheduler.get_lr()[0],acc_avg,acc,total1,loss_avg))\n",
    "        print(\"Epoch: \" + str(epoch) + \" Iter: \" + str(i) + \" Acc: \" + (\"%.2f\" % acc_avg) +\" Classification Loss: \" + str(loss_avg))\n",
    "        logging.info('======>>>>> Online epoch: #%d, lr=%.10f,Acc=%f,correctnum=%f,allnum=%f,avg_loss=%f  <<<<<======' %(epoch, scheduler.get_lr()[0],acc_avg,acc,total1,loss_avg))\n",
    "        logging.info(\"Epoch: \" + str(epoch) + \" Iter: \" + str(i) + \" Acc: \" + (\"%.2f\" % acc_avg) +\" Classification Loss: \" + str(loss_avg))\n",
    "        if ((epoch+1)%1==0 or epoch==opt.nepoch-1):\n",
    "            # evaluate mode\n",
    "            torch.cuda.synchronize()\n",
    "            netR.eval()\n",
    "            conf_mat = np.zeros([opt.Num_Class, opt.Num_Class])\n",
    "            conf_mat60 = np.zeros([20, 20])\n",
    "            acc = 0.0\n",
    "            loss_sigma = 0.0\n",
    "\n",
    "            with torch.no_grad():       \n",
    "                for i, data in enumerate(tqdm(val_loader)):\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "                    points4DV_T,label,v_name = data\n",
    "                    # print(v_name)\n",
    "                    points4DV_T,label = points4DV_T.cuda(),label.cuda()\n",
    "\n",
    "                    xt, yt = group_points_4DV_T_S(points4DV_T, opt)#(B*F)*4*Cen*K  (B*F)*4*Cen*1\n",
    "                    \n",
    "                    xt = xt.type(torch.FloatTensor)\n",
    "                    yt = yt.type(torch.FloatTensor)\n",
    "\n",
    "                    prediction = netR(xt,yt)\n",
    "\n",
    "                    loss = criterion(prediction,label)\n",
    "                    # print(label,prediction)\n",
    "                    _, predicted60 = torch.max(prediction.data[:,0:20], 1)\n",
    "                    _, predicted = torch.max(prediction.data, 1)\n",
    "                    # print(predicted60.data)\n",
    "                    loss_sigma += loss.item()\n",
    "\n",
    "                    for j in range(len(label)):\n",
    "                        cate_i = label[j].cpu().numpy()\n",
    "                        pre_i = predicted[j].cpu().numpy()\n",
    "                        conf_mat[cate_i, pre_i] += 1.0\n",
    "                        \n",
    "                        if cate_i<20:\n",
    "                            pre_i60 = predicted60[j].cpu().numpy()\n",
    "                            conf_mat60[cate_i, pre_i60] += 1.0\n",
    "                    # print(conf_mat)\n",
    "            print('MSR120:{:.2%} MSR60:{:.2%}--correct number {}--all number {}===Average loss:{:.6%}'.format(conf_mat.trace() / conf_mat.sum(),conf_mat60.trace() / conf_mat60.sum(),conf_mat60.trace(),conf_mat60.sum(),loss_sigma/(i+1)/2))\n",
    "            logging.info('#################{} --epoch{} set Accuracy:{:.2%}--correct number {}--all number {}===Average loss:{}'.format('Valid', epoch, conf_mat.trace() / conf_mat.sum(),conf_mat60.trace(),conf_mat60.sum(), loss_sigma/(i+1)))\n",
    "            Loss.append(loss_sigma/(i+1))\n",
    "            Correct.append(conf_mat60.sum())\n",
    "            Accuracy.append(conf_mat.trace() / conf_mat.sum())\n",
    "        torch.save(netR.module.state_dict(), '%s/pointnet_para_%d.pth' % (opt.save_root_dir, epoch))\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554d5681-8f88-4ae1-a332-79eef15116e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n",
      "Namespace(EACH_FRAME_SAMPLE_NUM=512, INPUT_FEATURE_NUM=3, Num_Class=6, SAMPLE_NUM=4096, Seg_size=1, T_ball_radius=0.2, T_knn_K=48, T_knn_K2=16, T_sample_num_level1=128, T_sample_num_level2=32, all_framenum=32, ball_radius=0.1, ball_radius2=0.2, batchSize=8, dataset='ntu60', framenum=32, gamma=0.5, knn_K=64, learning_rate=0.001, learning_rate_decay=1e-07, main_gpu=0, model='', momentum=0.9, nepoch=150, ngpu=1, optimizer='', pooling='concatenation', root_path='4DV/train_T', sample_num_level1=512, sample_num_level2=128, save_root_dir='xsub_ntu', size='full', stride=1, temperal_num=3, weight_decay=0.0008, workers=0)\n",
      "96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting video info: 100%|██████████| 96/96 [00:00<00:00, 142330.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_data: 48\n",
      "96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting video info: 100%|██████████| 96/96 [00:00<00:00, 70051.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_data: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): PointNet_Plus(\n",
      "    (netR_T_S1): Sequential(\n",
      "      (0): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=(1, 48), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (ca_S2): ChannelAttention(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc1): Conv2d(132, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (relu1): ReLU()\n",
      "      (fc2): Conv2d(8, 132, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "    (netR_T_S2): Sequential(\n",
      "      (0): Conv2d(132, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=(1, 16), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (ca_T1): ChannelAttention(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc1): Conv2d(259, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (relu1): ReLU()\n",
      "      (fc2): Conv2d(16, 259, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "    (net4DV_T1): Sequential(\n",
      "      (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=(1, 32), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (net4DV_T2): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (maxpoolings): ModuleList(\n",
      "      (0): MaxPool2d(kernel_size=(24, 1), stride=(24, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=(12, 1), stride=(12, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (netR_FC): Sequential(\n",
      "      (0): Linear(in_features=4352, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=256, out_features=6, bias=True)\n",
      "      (4): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/root/MSR/dataset/dataset.py\", line 109, in __getitem__\n    points4DV_T = np.load(path_cloud_npy_T)[frame_index, 0:self.EACH_FRAME_SAMPLE_NUM,\nIndexError: index 24 is out of bounds for axis 0 with size 24\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m Correct\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      8\u001b[0m Accuracy\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mCorrect\u001b[49m\u001b[43m,\u001b[49m\u001b[43mAccuracy\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 113\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(Loss, Correct, Accuracy)\u001b[0m\n\u001b[1;32m    110\u001b[0m total1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    111\u001b[0m timer \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader, \u001b[38;5;241m0\u001b[39m)):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tqdm/std.py:1185\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1182\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1187\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/root/MSR/dataset/dataset.py\", line 109, in __getitem__\n    points4DV_T = np.load(path_cloud_npy_T)[frame_index, 0:self.EACH_FRAME_SAMPLE_NUM,\nIndexError: index 24 is out of bounds for axis 0 with size 24\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.is_available())\n",
    "    \n",
    "    Loss=[]\n",
    "    Correct=[]\n",
    "    Accuracy=[]\n",
    "    main(Loss,Correct,Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce72ca-58c3-4403-ad1d-a12f90789b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve(150,Loss,'average_loss')\n",
    "curve(150,Correct,'Correct_number')\n",
    "curve(150,Accuracy,'accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
