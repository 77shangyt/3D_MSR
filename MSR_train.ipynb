{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9743b8-c0eb-4c46-8bc4-cfb034c6a51a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ede09d-a220-425a-bdfc-4d77a2118366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import shutil\n",
    "import collections\n",
    "import argparse\n",
    "import random\n",
    "import time\n",
    "#import gpu_utils as g\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from model import PointNet_Plus#,Attension_Point,TVLAD\n",
    "from utils import group_points_4DV_T_S\n",
    "from dataset import NTU_RGBD\n",
    "import train\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab390434-644e-43a1-855e-b1fde31650a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "#设置中文显示\n",
    "from pylab import mpl\n",
    "def curve(epoch,y,name1):\n",
    "    mpl.rcParams['font.sans-serif'] = ['SimHei'] \n",
    "    x=list(range(epoch))\n",
    "    plt.figure(figsize=(20,8),dpi=100)\n",
    "    plt.plot(x,y,color='r',linestyle='--')\n",
    "    #设置刻度[::5]切片\n",
    "    plt.xticks(x[::5])\n",
    "    plt.yticks(y[::5])\n",
    "    #添加描述信息\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(name1)\n",
    "    plt.title(name1)\n",
    "    #添加网格线\n",
    "    plt.grid(True,linestyle='--',alpha=0.5)\n",
    "    #4.保存图片\n",
    "    name='result/'+name1\n",
    "    plt.savefig(name)\n",
    "    #5.显示图像(4,5步顺序不能调换，如果4,5步调换顺序，则图片无法保存）\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9359398a-0d12-4c9f-aafa-d4b92562c155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(Loss,Correct,Accuracy):\n",
    "    parser = argparse.ArgumentParser(description = \"Training\")\n",
    "\n",
    "    parser.add_argument('--batchSize', type=int, default=16, help='input batch size')#￥￥￥￥\n",
    "    parser.add_argument('--nepoch', type=int, default=150, help='number of epochs to train for')\n",
    "    parser.add_argument('--INPUT_FEATURE_NUM', type=int, default = 3,  help='number of input point features')\n",
    "    parser.add_argument('--temperal_num', type=int, default = 3,  help='number of input point features')\n",
    "    parser.add_argument('--pooling', type=str, default='concatenation', help='how to aggregate temporal split features: vlad | concatenation | bilinear')\n",
    "    parser.add_argument('--dataset', type=str, default='ntu60', help='how to aggregate temporal split features: ntu120 | ntu60')\n",
    "\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.0008, help='weight decay (SGD only)')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.001, help='learning rate at t=0')#￥￥￥￥\n",
    "    parser.add_argument('--gamma', type=float, default=0.5, help='')#￥￥￥￥\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='momentum (SGD only)')\n",
    "    parser.add_argument('--workers', type=int, default=0, help='number of data loading workers')\n",
    "\n",
    "    parser.add_argument('--root_path', type=str, default='01_MSR3D',  help='preprocess folder')\n",
    "    #parser.add_argument('--root_path', type=str, default='4DV/train(24_2048_3)',  help='preprocess folder')\n",
    "    # parser.add_argument('--depth_path', type=str, default='C:\\\\Users\\\\Administrator\\\\Desktop\\\\LX\\paper\\\\dataset\\\\Prosessed_dataset\\\\01_MSR3D\\\\',  help='raw_depth_png')\n",
    "    ################\n",
    "    # parser.add_argument('--save_root_dir', type=str, default='C:\\\\Users\\\\Administrator\\\\Desktop\\\\LX\\\\paper\\\\code\\\\3DV-Action-master\\\\models\\\\ntu60\\\\xsub',  help='output folder')\n",
    "    #parser.add_argument('--save_root_dir', type=str, default='xsub/xsub_ntu_24_2048',  help='output folder')\n",
    "    parser.add_argument('--save_root_dir', type=str, default='xsub',  help='output folder')\n",
    "    parser.add_argument('--model', type=str, default = '',  help='model name for training resume')\n",
    "    parser.add_argument('--optimizer', type=str, default = '',  help='optimizer name for training resume')\n",
    "    \n",
    "    parser.add_argument('--ngpu', type=int, default=1, help='# GPUs')\n",
    "    parser.add_argument('--main_gpu', type=int, default=0, help='main GPU id') # CUDA_VISIBLE_DEVICES=0 python train.py\n",
    "\n",
    "    ########\n",
    "    parser.add_argument('--Seg_size', type=int, default =1,  help='number of frame in seg')\n",
    "    parser.add_argument('--stride', type=int, default = 1,  help='stride of seg')\n",
    "    parser.add_argument('--all_framenum', type=int, default = 24,  help='number of action frame')\n",
    "    parser.add_argument('--framenum', type=int, default = 24,  help='number of action frame')\n",
    "    parser.add_argument('--EACH_FRAME_SAMPLE_NUM', type=int, default = 512,  help='number of sample points in each frame')\n",
    "    parser.add_argument('--T_knn_K', type=int, default = 48,  help='K for knn search of temperal stream')\n",
    "    parser.add_argument('--T_knn_K2', type=int, default = 16,  help='K for knn search of temperal stream')\n",
    "    parser.add_argument('--T_sample_num_level1', type=int, default = 128,  help='number of first layer groups')\n",
    "    parser.add_argument('--T_sample_num_level2', type=int, default = 32,  help='number of first layer groups')\n",
    "    parser.add_argument('--T_ball_radius', type=float, default=0.2, help='square of radius for ball query of temperal stream')\n",
    "    \n",
    "    parser.add_argument('--learning_rate_decay', type=float, default=1e-7, help='learning rate decay')\n",
    "\n",
    "    parser.add_argument('--size', type=str, default='full', help='how many samples do we load: small | full')\n",
    "    parser.add_argument('--SAMPLE_NUM', type=int, default = 2048,  help='number of sample points')\n",
    "\n",
    "    parser.add_argument('--Num_Class', type=int, default = 20,  help='number of outputs')\n",
    "    parser.add_argument('--knn_K', type=int, default = 64,  help='K for knn search')\n",
    "    parser.add_argument('--sample_num_level1', type=int, default = 512,  help='number of first layer groups')\n",
    "    parser.add_argument('--sample_num_level2', type=int, default = 128,  help='number of second layer groups')\n",
    "    parser.add_argument('--ball_radius', type=float, default=0.1, help='square of radius for ball query in level 1')#0.025 -> 0.05 for detph\n",
    "    parser.add_argument('--ball_radius2', type=float, default=0.2, help='square of radius for ball query in level 2')# 0.08 -> 0.01 for depth\n",
    "\n",
    "\n",
    "    opt = parser.parse_args(args=[])\n",
    "    print(opt)\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%Y/%m/%d %H:%M:%S', filename=os.path.join(opt.save_root_dir, 'train00.log'), level=logging.INFO)\n",
    "    # torch.cuda.set_device(opt.main_gpu)\n",
    "\n",
    "    opt.manualSeed = 1\n",
    "    random.seed(opt.manualSeed)\n",
    "    torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(opt.save_root_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    torch.cuda.empty_cache()\n",
    "    ##############################\n",
    "    data_train = NTU_RGBD(root_path = opt.root_path,opt=opt,\n",
    "        DATA_CROSS_VIEW = False,\n",
    "        full_train = True,\n",
    "        validation = False,\n",
    "        test = False,\n",
    "        Transform = True\n",
    "        )\n",
    "    train_loader = DataLoader(dataset = data_train, batch_size = opt.batchSize, shuffle = True, drop_last = True,num_workers = 8)\n",
    "    data_val = NTU_RGBD(root_path = opt.root_path, opt=opt,\n",
    "        DATA_CROSS_VIEW = False,\n",
    "        full_train = False,\n",
    "        validation = False,\n",
    "        test = True,\n",
    "        Transform = False\n",
    "        )\n",
    "    val_loader = DataLoader(dataset = data_val, batch_size = 24,num_workers = 8)\n",
    "\n",
    "    netR = PointNet_Plus(opt)\n",
    "\n",
    "    netR = torch.nn.DataParallel(netR).cuda()\n",
    "    netR.cuda()\n",
    "    print(netR)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.Adam(netR.parameters(), lr=opt.learning_rate, betas = (0.5, 0.999), eps=1e-06)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=opt.gamma)\n",
    "\n",
    "    for epoch in range(opt.nepoch):\n",
    "        scheduler.step()\n",
    "        #print(epoch)\n",
    "        # switch to train mode\n",
    "        torch.cuda.synchronize()\n",
    "        netR.train()\n",
    "        acc = 0.0\n",
    "        loss_sigma = 0.0\n",
    "        total1 = 0.0\n",
    "        timer = time.time()\n",
    "        \n",
    "        for i, data in enumerate(tqdm(train_loader, 0)):\n",
    "            if len(data[0])==1:\n",
    "                continue\n",
    "            torch.cuda.synchronize()\n",
    "            # 1 load imputs and target\n",
    "            ## 3DV points and 3 temporal segment appearance points\n",
    "            ## points_xyzc: B*4096*8;points_1xyz:B*2048*3  target: B*1\n",
    "            points4DV_T,label,v_name = data\n",
    "            points4DV_T,label = points4DV_T.cuda(),label.cuda()\n",
    "            # print('points4DV_T:',points4DV_T.shape)\n",
    "            xt, yt = group_points_4DV_T_S(points4DV_T, opt)#B*F*4*Cen*K  B*F*4*Cen*1\n",
    "            # print('xt:',xt.shape)\n",
    "            xt = xt.type(torch.FloatTensor)\n",
    "            yt = yt.type(torch.FloatTensor)\n",
    "\n",
    "            prediction = netR(xt,yt)\n",
    "\n",
    "            loss = criterion(prediction,label)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            # update training error\n",
    "            loss_sigma += loss.item()\n",
    "            #_, predicted60 = torch.max(prediction.data[:,0:60], 1)\n",
    "            _, predicted = torch.max(prediction.data, 1)\n",
    "            # print(predicted.data)\n",
    "            acc += (predicted==label).cpu().sum().numpy()\n",
    "            total1 += label.size(0)\n",
    "        \n",
    "        \n",
    "        acc_avg = acc/total1\n",
    "        loss_avg = loss_sigma/total1\n",
    "        print('======>>>>> Online epoch: #%d, lr=%.10f,Acc=%f,correctnum=%f,allnum=%f,avg_loss=%f  <<<<<======' %(epoch, scheduler.get_lr()[0],acc_avg,acc,total1,loss_avg))\n",
    "        print(\"Epoch: \" + str(epoch) + \" Iter: \" + str(i) + \" Acc: \" + (\"%.2f\" % acc_avg) +\" Classification Loss: \" + str(loss_avg))\n",
    "        logging.info('======>>>>> Online epoch: #%d, lr=%.10f,Acc=%f,correctnum=%f,allnum=%f,avg_loss=%f  <<<<<======' %(epoch, scheduler.get_lr()[0],acc_avg,acc,total1,loss_avg))\n",
    "        logging.info(\"Epoch: \" + str(epoch) + \" Iter: \" + str(i) + \" Acc: \" + (\"%.2f\" % acc_avg) +\" Classification Loss: \" + str(loss_avg))\n",
    "        if ((epoch+1)%1==0 or epoch==opt.nepoch-1):\n",
    "            # evaluate mode\n",
    "            torch.cuda.synchronize()\n",
    "            netR.eval()\n",
    "            conf_mat = np.zeros([opt.Num_Class, opt.Num_Class])\n",
    "            conf_mat60 = np.zeros([20, 20])\n",
    "            acc = 0.0\n",
    "            loss_sigma = 0.0\n",
    "\n",
    "            with torch.no_grad():       \n",
    "                for i, data in enumerate(tqdm(val_loader)):\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "                    points4DV_T,label,v_name = data\n",
    "                    # print(v_name)\n",
    "                    points4DV_T,label = points4DV_T.cuda(),label.cuda()\n",
    "\n",
    "                    xt, yt = group_points_4DV_T_S(points4DV_T, opt)#(B*F)*4*Cen*K  (B*F)*4*Cen*1\n",
    "                    \n",
    "                    xt = xt.type(torch.FloatTensor)\n",
    "                    yt = yt.type(torch.FloatTensor)\n",
    "\n",
    "                    prediction = netR(xt,yt)\n",
    "\n",
    "                    loss = criterion(prediction,label)\n",
    "                    # print(label,prediction)\n",
    "                    _, predicted60 = torch.max(prediction.data[:,0:20], 1)\n",
    "                    _, predicted = torch.max(prediction.data, 1)\n",
    "                    # print(predicted60.data)\n",
    "                    loss_sigma += loss.item()\n",
    "\n",
    "                    for j in range(len(label)):\n",
    "                        cate_i = label[j].cpu().numpy()\n",
    "                        pre_i = predicted[j].cpu().numpy()\n",
    "                        conf_mat[cate_i, pre_i] += 1.0\n",
    "                        \n",
    "                        if cate_i<20:\n",
    "                            pre_i60 = predicted60[j].cpu().numpy()\n",
    "                            conf_mat60[cate_i, pre_i60] += 1.0\n",
    "                    # print(conf_mat)\n",
    "            print('MSR120:{:.2%} MSR60:{:.2%}--correct number {}--all number {}===Average loss:{:.6%}'.format(conf_mat.trace() / conf_mat.sum(),conf_mat60.trace() / conf_mat60.sum(),conf_mat60.trace(),conf_mat60.sum(),loss_sigma/(i+1)/2))\n",
    "            logging.info('#################{} --epoch{} set Accuracy:{:.2%}--correct number {}--all number {}===Average loss:{}'.format('Valid', epoch, conf_mat.trace() / conf_mat.sum(),conf_mat60.trace(),conf_mat60.sum(), loss_sigma/(i+1)))\n",
    "            Loss.append(loss_sigma/(i+1))\n",
    "            Correct.append(conf_mat60.sum())\n",
    "            Accuracy.append(conf_mat.trace() / conf_mat.sum())\n",
    "        torch.save(netR.module.state_dict(), '%s/pointnet_para_%d.pth' % (opt.save_root_dir, epoch))\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f05d78-b771-43b7-9958-c1c223c17105",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n",
      "Namespace(EACH_FRAME_SAMPLE_NUM=512, INPUT_FEATURE_NUM=3, Num_Class=20, SAMPLE_NUM=2048, Seg_size=1, T_ball_radius=0.2, T_knn_K=48, T_knn_K2=16, T_sample_num_level1=128, T_sample_num_level2=32, all_framenum=24, ball_radius=0.1, ball_radius2=0.2, batchSize=16, dataset='ntu60', framenum=24, gamma=0.5, knn_K=64, learning_rate=0.001, learning_rate_decay=1e-07, main_gpu=0, model='', momentum=0.9, nepoch=150, ngpu=1, optimizer='', pooling='concatenation', root_path='01_MSR3D', sample_num_level1=512, sample_num_level2=128, save_root_dir='xsub', size='full', stride=1, temperal_num=3, weight_decay=0.0008, workers=0)\n",
      "557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting video info: 100%|██████████| 557/557 [00:00<00:00, 243814.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_data: 284\n",
      "557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting video info: 100%|██████████| 557/557 [00:00<00:00, 225005.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_data: 273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): PointNet_Plus(\n",
      "    (netR_T_S1): Sequential(\n",
      "      (0): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=(1, 48), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (ca_S2): ChannelAttention(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc1): Conv2d(132, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (relu1): ReLU()\n",
      "      (fc2): Conv2d(8, 132, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "    (netR_T_S2): Sequential(\n",
      "      (0): Conv2d(132, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=(1, 16), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (ca_T1): ChannelAttention(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc1): Conv2d(259, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (relu1): ReLU()\n",
      "      (fc2): Conv2d(16, 259, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "    (net4DV_T1): Sequential(\n",
      "      (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): ReLU(inplace=True)\n",
      "      (9): MaxPool2d(kernel_size=(1, 32), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (net4DV_T2): Sequential(\n",
      "      (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (maxpoolings): ModuleList(\n",
      "      (0): MaxPool2d(kernel_size=(24, 1), stride=(24, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): MaxPool2d(kernel_size=(12, 1), stride=(12, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (netR_FC): Sequential(\n",
      "      (0): Linear(in_features=4352, out_features=256, bias=True)\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Linear(in_features=256, out_features=20, bias=True)\n",
      "      (4): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:05<00:00,  3.29it/s]\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:389: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======>>>>> Online epoch: #0, lr=0.0010000000,Acc=0.257353,correctnum=70.000000,allnum=272.000000,avg_loss=0.165490  <<<<<======\n",
      "Epoch: 0 Iter: 16 Acc: 0.26 Classification Loss: 0.16549013204434337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSR120:5.49% MSR60:5.49%--correct number 15.0--all number 273.0===Average loss:172.615437%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.is_available())\n",
    "    \n",
    "    Loss=[]\n",
    "    Correct=[]\n",
    "    Accuracy=[]\n",
    "    main(Loss,Correct,Accuracy)\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b708a7-2ced-4eea-ac32-a077d6c34fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve(150,Loss,'average_loss')\n",
    "curve(150,Correct,'Correct_number')\n",
    "curve(150,Accuracy,'accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
